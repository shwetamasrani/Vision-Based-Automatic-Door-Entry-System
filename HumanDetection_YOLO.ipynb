{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = cv.dnn.readNet(\"./config/yolov3.weights\", \"./config/yolov3.cfg\")\n",
    "classes = []\n",
    "with open(\"./data/coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_dict={0:'MASK',1:'NO MASK'}\n",
    "color_dict={0:(0,255,0),1:(0,0,255)}\n",
    "label_test = np.array([0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=200, kernel_size=(3,3))\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=200, out_channels=100, kernel_size=(2,2))\n",
    "        self.Dropout = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(in_features=24*24*100, out_features=50)\n",
    "        self.fc2 = nn.Linear(in_features=50, out_features=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x)) #98*98*200\n",
    "        x = self.pool(x) #49*49*200\n",
    "        x = F.relu(self.conv2(x)) #48*48*100\n",
    "        x = self.pool(x) #24*24*100\n",
    "        x = self.Dropout(x)\n",
    "        x = x.view(-1, 24 * 24 * 100)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.softmax(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "convNet = ConvNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNet(\n",
       "  (conv1): Conv2d(3, 200, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(200, 100, kernel_size=(2, 2), stride=(1, 1))\n",
       "  (Dropout): Dropout(p=0.5, inplace=False)\n",
       "  (fc1): Linear(in_features=57600, out_features=50, bias=True)\n",
       "  (fc2): Linear(in_features=50, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.load('./config/model95.pth', map_location='cpu')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video Capturing flag = True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PRATAP\\miniconda3\\envs\\MLEnv\\lib\\site-packages\\ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "# Loading image\n",
    "cap = cv.VideoCapture('./Videos/Video1.mp4')\n",
    "print('Video Capturing flag =',cap.isOpened())\n",
    "while True :\n",
    "    _, frame = cap.read()\n",
    "    height, width, channels = frame.shape\n",
    "    \n",
    "    # Detecting objects\n",
    "    blob = cv.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(output_layers)\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if confidence > 0.5 and class_id == 0:\n",
    "                # Object detected\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                w = int(detection[2] * width)\n",
    "                h = int(detection[3] * height)\n",
    "                # Rectangle coordinates\n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "                boxes.append([x, y, w, h])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "    indexes = cv.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "    font = cv.FONT_HERSHEY_PLAIN\n",
    "    for i in range(len(boxes)):\n",
    "        if i in indexes:\n",
    "            x, y, w, h = boxes[i]\n",
    "            label = str(classes[class_ids[i]])\n",
    "            color = (255, 0, 0)\n",
    "            cv.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
    "            cv.putText(frame, label, (x, y + 30), font, 3, color, 3)\n",
    "            faceCascade = cv.CascadeClassifier('./config/haarcascade_frontalface_default.xml')\n",
    "            gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)\n",
    "            faces = faceCascade.detectMultiScale(gray,\n",
    "                                         scaleFactor=1.1,\n",
    "                                         minNeighbors=5,\n",
    "                                         minSize=(60, 60),\n",
    "                                         flags=cv.CASCADE_SCALE_IMAGE)\n",
    "            for (x, y ,w, h) in faces:\n",
    "                \n",
    "                face_img = frame[y:y+w, x:x+w]\n",
    "                resized = cv.resize(face_img,(100,100))\n",
    "                filename = './Test/testing/test_image.jpg'\n",
    "                cv.imwrite(filename, resized)\n",
    "                transform = transforms.Compose([\n",
    "                        transforms.Resize(256),\n",
    "                        transforms.CenterCrop(100),\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                    ])\n",
    "                test_dataset = torchvision.datasets.ImageFolder('./Test', transform=transform)\n",
    "                testloader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "                with torch.no_grad():\n",
    "                    for i, data in enumerate(testloader, 0):\n",
    "                        images, labels = data[0], data[1]\n",
    "                        outputs = model(images)\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        predicted = predicted.numpy()\n",
    "                        if np.sum(predicted == label_test) == 1:\n",
    "                            cv.rectangle(frame, (x, y), (x + w, y + h),(0, 255, 0), 2)\n",
    "                            cv.putText(frame, \"Face:With Mask\", (x, y + 2), font, 3, (0, 255, 0), 2)\n",
    "                        else :\n",
    "                            cv.rectangle(frame, (x, y), (x + w, y + h),(0, 255, 0), 2)\n",
    "                            cv.putText(frame, \"Face:Without Mask\", (x, y + 2), font, 3, (0, 0, 255), 2)\n",
    "                        break\n",
    "                \n",
    "    cv.imshow(\"Image\", frame)\n",
    "    key = cv.waitKey(1)\n",
    "    if key == 27:\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
